# Implementation Plan: Incident Creation Agent

## 1. Objective

Create a new agent in the `agents\incident_creation\` folder. This agent will be responsible for creating detailed
incident reports in Jira based on test execution results. Additionally,modify the **Orchestrator** to collect the
required context (logs, media, system info) and invoke this agent upon each test failure.

## 2. Input Data Requirements

The **Orchestrator** must be extended to provide the following data to the agent for a single test case:

* **Test Execution Results**: Status, error messages, stack traces.
* **Agent Execution Logs**: Detailed logs of the agent's actions during the test.
* **Screenshots or Videos**: Paths or content of visual evidence.
* **System Description**: Description of the system on which the agent executed the test case.
    * Format example: `Device: iPhone 14, OS Version: iOS 17.1, Browser: Safari v17, Environment: QA Staging`.

## 3. Incident Report Specifications

The incident report generated by the agent must contain:

1. **Title / Summary**:
    * Format: `[Feature/Module] - Short Description of Failure`
    * Content: A concise summary of the defect.
2. **Description**:
    * Content: Brief overview of context, specific nature of failure, and the last execution step with its data.
3. **Environment Details**:
    * Device, OS Version, Browser, Environment.
4. **Steps to Reproduce**:
    * Format: Numbered, step-by-step guide.
    * Content: All steps executed *before* and *including* the failure.
5. **Expected Result**: Result of the failed step.
6. **Actual Result**: Actual result of the failed step.
7. **Severity**: `Critical`, `Major`, `Minor`, `Cosmetic`.
8. **Priority**: `High`, `Medium`, `Low`.
9. **Attachments**: Screenshots, videos, execution logs.

## 4. Agent Logic & Workflow

1. **Task Reception**: Receive `IncidentCreationInput` from the Orchestrator.
2. **Duplicate Detection (RAG)**:
    * Search RAG with Jira issues (bugs) for existing bugs matching the failure. Use the pydantic ai example for
      pgvector (https://ai.pydantic.dev/examples/rag/) in order to correctly implement the corresponding agent search
      functionality. Also take into account the RAG config.
    * Check bug tickets linked to the original Test Case.
    * Fetch content of potential duplicates and use LLM to compare and decide.
    * The comparison of the current incident and each of the fetched duplicate candidates and decision (duplicate or
      not) must be done by the sub-agent, one-by-one. The correct way to use sub-agents can be found on the example of
      test case generation agent.
3. **Jira Interaction (MCP)**:
    * Retrieve the **Test Case** and **Linked Bugs**.
    * If no duplicate is found, create a new Jira incident.
4. **Incident Creation**:
    * Generate report content (Title, Description, Steps, etc.).
    * Create incident and link to the original Test Case and attach evidence (logs, screenshots, videos) via MCP.

## 5. Technical Implementation: Agent Side (`agents/incident_creation/`)

### Directory Structure

```text
agents/
    incident_creation/
        ├── Dockerfile          # Standard agent Dockerfile
        ├── main.py             # Agent entry point and logic
        ├── prompt.py           # System prompt definition
        └── prompt_template.txt # Detailed system instructions
```

### Data Models (`common/models.py`)

**Input Model:**

```python
class IncidentCreationInput(JsonSerializableModel):
    test_case_key: str
    test_execution_result: str
    agent_execution_logs: Optional[str] = None
    system_description: str
    available_artefacts: List[FileWithBytes]
```

**Output Model:**

```python
class IncidentCreationResult(JsonSerializableModel):
    incident_key: str = Field(description="The key of the created incident, may be null if duplicates are detected")
    duplicates: list[DuplicateDetectionResult] = Field(
        description="All identified positive duplicate incident detection results, may be empty")
```

**Duplicate Detection Output Model:**

```python
class DuplicateDetectionResult(JsonSerializableModel):
    issue_key: str = Field(description="The key of existing incident Jira issue, which is a candidate for duplicate")
    is_duplicate: bool = Field(description="True if the candidate is indeed a duplicate")
    message: str = Field(description="Elaborate Justification of the decision about being or not being a duplicate")
```

### System Prompt of the main agent (`main_agent_prompt_template.txt`)

* Role: Expert Software QA specialized in reporting incidents.
* Instructions: RAG for duplicates retrieval, then creating incident report with strict field formatting,
  Severity/Priority logic.

### System Prompt of the sub-agent for duplicate detection (`duplicate_detection_prompt_template.txt`)

* Role: Expert Software QA specialized in Analysis of Bug reports and duplicate detection.
* Instructions: Comparison of two incidents (bugs) and decision if they describe the same incident/bug.
* Also put in this prompt the best practices for identifying if the current incident is a duplicate of any other
  incident (use web search for that).

## 6. Technical Implementation: Orchestrator Side (`orchestrator/`)

To comply with the requirement that the Orchestrator provides the data, the following modifications are required in the
Orchestrator's codebase:

### 6.1. Execution Loop Modifications (`_orchestrate_step_execution`)

The execution logic must be updated to capture context *during* the test execution phase so it is available if a failure
occurs.

1. **Context Capture**:
    * Modify the loop to continuously aggregate or buffer **Agent Execution Logs** for the current test execution.
    * Extend the existing TestExecutionResult to have the **System Description** and execution logs.

2. **Failure Handling & Invocation**:
    * Detect test failure within the execution step (e.g., checking `progress_ledger["is_current_step_complete"]` or
      specific error flags).
    * **If failure detected**:
        * Instantiate `IncidentCreationInput` with correct input.
        * Invoke the `IncidentCreationAgent` (similar to how it's already done with all other agents).
        * Await `IncidentCreationResult`.
        * Analyze the `IncidentCreationResult` and retrieve the information from it: if incident was created, get its
          key, otherwise get the duplicates list.
        * Pass this information to the client which creates test execution results in the test management system
3. **Test execution results in test management system**:
    * The test management system client must be extended to get the created incident report info passed by the
      orchestrator and add it to the comments of the execution. The current implementation already adds some
      failure/error info as a comment to the test execution result, it simply needs to add incident report info as the
      last paragraph of this comment.

## 7. Configuration (`config.py`)

* **IncidentCreationAgentConfig**:
    * `PORT`, `EXTERNAL_PORT`.
    * `MODEL_NAME` (e.g., `gemini-2.5-flash`).
    * `THINKING_BUDGET` (e.g., 16000).
* **OrchestratorConfig**:
    * Update to include references/URLs for the new agent.
* **RAG Config**:
    * Create the config for the RAG (URL, user, password, table name, min similarity score for bug ticket retrieval
      etc.)

## 8. RAG

RAG is to be implemented using pydantic-ai with pgvector. All the corresponding config must be present.

You must also create a new agent and corresponding endpoint (which receives the Jira project issue key) in orchestrator
responsible for keeping the vector DB up-to
date. The endpoint must be blocked with a lock ensuring it can't be triggered concurrently.
After its triggering, this agent must retrieve the most recent data update timestamp in vector DB for this project
key (this info must
be saved in the same vector DB, but in a different table which name is a config prop), use this timestamp as the start
period, create 3 JQL queries to find created, updated and deleted issues with valid statuses (the list of Jira issue
statuses which are valid needs to be configurable) of type Bug (the actual name of Jira issue must be as config prop),
use MCP server to find all issues for those 3 queries and then do the corresponding operations in the vector DB: create,
update or delete those issues based on Jira issue ID. Only Jira issue ID and issue content need to be fetched from MCP
server in case of update or creation, then Jira issue ID and vector embeddings of its content need to be stored or
updated in the vector DB saved. For deleting issues simply find the issue based on its ID.
